{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brayden-Zhang/Climate-Change-AI-Summer-School-2024/blob/main/Detecting_road_and_roadtypes_in_sattelite_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXY_MTc2CLGq"
      },
      "source": [
        "For a full explanation of the code, visit http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install owslib\n",
        "!pip install shapefile\n",
        "\n",
        "!pip install cv2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai0rkOaJFcwA",
        "outputId": "82027032-5763-4c06-efc6-70769a8de890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: owslib in /usr/local/lib/python3.7/dist-packages (0.26.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from owslib) (6.0)\n",
            "Requirement already satisfied: requests>=1.0 in /usr/local/lib/python3.7/dist-packages (from owslib) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from owslib) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=1.5 in /usr/local/lib/python3.7/dist-packages (from owslib) (2.8.2)\n",
            "Requirement already satisfied: pyproj<3.3.0 in /usr/local/lib/python3.7/dist-packages (from owslib) (3.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from pyproj<3.3.0->owslib) (2022.6.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=1.5->owslib) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0->owslib) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0->owslib) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0->owslib) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shapefile (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for shapefile\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "NQFcs_2jCLG5",
        "outputId": "a00cf79d-0252-4a30-c50b-d93a577b29c4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-955044d51237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#pyshp is necessary for loading and saving shapefiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#install with 'pip install pyshp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshapefile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Install opencv with 'pip install opencv-python'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shapefile'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'..')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "from scipy import ndimage\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#We are using owslib to download images from a WMS Service\n",
        "#install with 'pip install owslib'\n",
        "from owslib.wms import WebMapService\n",
        "\n",
        "#pyshp is necessary for loading and saving shapefiles\n",
        "#install with 'pip install pyshp'\n",
        "import shapefile\n",
        "\n",
        "# Install opencv with 'pip install opencv-python'\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tceT5-6VCLHI"
      },
      "outputs": [],
      "source": [
        "x_min = 90000\n",
        "y_min = 427000\n",
        "dx, dy = 200, 200\n",
        "no_tiles_x = 100\n",
        "no_tiles_y = 100\n",
        "total_no_tiles = no_tiles_x * no_tiles_y\n",
        "\n",
        "x_max = x_min + no_tiles_x * dx\n",
        "y_max = y_min + no_tiles_y * dy\n",
        "bounding_box = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "TILE_FOLDER = \"../datasets/image_tiles_200/\"\n",
        "URL_TILES = \"https://geodata.nationaalgeoregister.nl/luchtfoto/rgb/wms?request=GetCapabilities\"\n",
        "\n",
        "URL_SHP = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shp'\n",
        "URL_PRF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.prj'\n",
        "URL_DBF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.dbf'\n",
        "URL_SHX = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shx'\n",
        "\n",
        "URLS_SHAPEFILES = [URL_SHP, URL_PRF, URL_DBF, URL_SHX]\n",
        "\n",
        "DATA_FOLDER = \"../data/nwb_wegvakken/\"\n",
        "\n",
        "json_filename = DATA_FOLDER + '2017_09_wegvakken.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJApvbwQCLHL"
      },
      "source": [
        "# 1. Downloading the image tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "qduvkBeWCLHN",
        "outputId": "5033aeee-33dd-4270-c27f-2e895b0d5485"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0ac7699e1993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebMapService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL_TILES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.1.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTILE_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTILE_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/owslib/wms.py\u001b[0m in \u001b[0;36mWebMapService\u001b[0;34m(url, version, xml, username, password, parse_remote_metadata, timeout, headers, auth)\u001b[0m\n\u001b[1;32m     50\u001b[0m         return wms111.WebMapService_1_1_1(\n\u001b[1;32m     51\u001b[0m             \u001b[0mclean_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_remote_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_remote_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             timeout=timeout, headers=headers, auth=auth)\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'1.3.0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         return wms130.WebMapService_1_3_0(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/owslib/map/wms111.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, version, xml, username, password, parse_remote_metadata, headers, timeout, auth)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# read from server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/owslib/map/common.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, service_url, timeout)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mspliturl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         u = openURL(spliturl[0], spliturl[1], method='Get',\n\u001b[0;32m---> 66\u001b[0;31m                     timeout=timeout, headers=self.headers, auth=self.auth)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/owslib/util.py\u001b[0m in \u001b[0;36mopenURL\u001b[0;34m(url_base, data, method, cookies, username, password, timeout, headers, verify, cert, auth)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m404\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m502\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m503\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m504\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# add more if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# check for service exceptions without the http header set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error:  for url: https://geodata.nationaalgeoregister.nl/luchtfoto/rgb/wms?service=WMS&request=GetCapabilities&version=1.1.1"
          ]
        }
      ],
      "source": [
        "wms = WebMapService(URL_TILES, version='1.1.1')\n",
        "\n",
        "if not os.path.exists(TILE_FOLDER):\n",
        "    os.makedirs(TILE_FOLDER)\n",
        "\n",
        "for ii in range(25,no_tiles_x):\n",
        "    print(ii)\n",
        "    for jj in range(0,no_tiles_y):\n",
        "        ll_x_ = x_min + ii*dx\n",
        "        ll_y_ = y_min + jj*dy\n",
        "        bbox = (ll_x_, ll_y_, ll_x_ + dx, ll_y_ + dy)\n",
        "        img = wms.getmap(layers=['Actueel_ortho25'], srs='EPSG:28992', bbox=bbox, size=(256, 256), format='image/jpeg', transparent=True)\n",
        "        filename = \"{}{}_{}_{}_{}.jpg\".format(TILE_FOLDER, bbox[0], bbox[1], bbox[2], bbox[3])\n",
        "        out = open(filename, 'wb')\n",
        "        out.write(img.read())\n",
        "        out.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtTlrevCLHW"
      },
      "source": [
        "## 1b. Downloading the shapefiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O733ZxWfCLHY"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DATA_FOLDER):\n",
        "    os.makedirs(DATA_FOLDER)\n",
        "\n",
        "for url in URLS_SHAPEFILES:\n",
        "    filename = url.split('/')[-1]\n",
        "    print(\"Downloading file {}\".format(filename))\n",
        "    r = requests.get(url, stream=True)\n",
        "    if r.status_code == 200:\n",
        "        with open(DATA_FOLDER + filename, 'wb') as f:\n",
        "            r.raw.decode_content = True\n",
        "            shutil.copyfileobj(r.raw, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL3inLCZCLHd"
      },
      "source": [
        "# 2. Loading shapefile and converting to (GEO)Json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v6hmY-DQCLHe"
      },
      "outputs": [],
      "source": [
        "def json_serial(obj):\n",
        "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
        "\n",
        "    if isinstance(obj, (datetime, date)):\n",
        "        serial = obj.isoformat()\n",
        "        return serial\n",
        "    if isinstance(obj, bytes):\n",
        "        return {'__class__': 'bytes',\n",
        "                '__value__': list(obj)}\n",
        "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
        "\n",
        "reader = shapefile.Reader(DATA_FOLDER + 'Wegvakken.shp')\n",
        "fields = reader.fields[1:]\n",
        "field_names = [field[0] for field in fields]\n",
        "\n",
        "buffer = []\n",
        "for sr in reader.shapeRecords()[:500000]:\n",
        "    atr = dict(zip(field_names, sr.record))\n",
        "    geom = sr.shape.__geo_interface__\n",
        "    buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr))\n",
        "\n",
        "\n",
        "json_file = open(json_filename, \"w\")\n",
        "json_file.write(json.dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2, default=json_serial) + \"\\n\")\n",
        "json_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVhmVMesCLHl"
      },
      "source": [
        "# 3. Declaring some variables and methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FQXRvB7_CLHq"
      },
      "outputs": [],
      "source": [
        "dict_roadtype = {\n",
        "    \"G\": 'Gemeente',\n",
        "    \"R\": 'Rijk',\n",
        "    \"P\": 'Provincie',\n",
        "    \"W\": 'Waterschap',\n",
        "    'T': 'Andere wegbeheerder',\n",
        "    '' : 'leeg'\n",
        "}\n",
        "\n",
        "dict_roadtype_to_color = {\n",
        "    \"G\": 'red',\n",
        "    \"R\": 'blue',\n",
        "    \"P\": 'green',\n",
        "    \"W\": 'magenta',\n",
        "    'T': 'yellow',\n",
        "    '' : 'leeg'\n",
        "}\n",
        "\n",
        "FEATURES_KEY = 'features'\n",
        "PROPERTIES_KEY = 'properties'\n",
        "GEOMETRY_KEY = 'geometry'\n",
        "COORDINATES_KEY = 'coordinates'\n",
        "WEGSOORT_KEY = 'WEGBEHSRT'\n",
        "\n",
        "MINIMUM_NO_POINTS_PER_TILE = 4\n",
        "POINTS_PER_METER = 0.1\n",
        "\n",
        "INPUT_FOLDER_TILES = './data/image_tiles_200/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e5_gvsLeCLHs"
      },
      "outputs": [],
      "source": [
        "def add_to_dict(d1, d2, coordinates, rtype):\n",
        "    coordinate_ll_x = int((coordinates[0] // dx)*dx)\n",
        "    coordinate_ll_y = int((coordinates[1] // dy)*dy)\n",
        "    coordinate_ur_x = int((coordinates[0] // dx)*dx + dx)\n",
        "    coordinate_ur_y = int((coordinates[1] // dy)*dy + dy)\n",
        "    tile = \"{}_{}_{}_{}.jpg\".format(coordinate_ll_x, coordinate_ll_y, coordinate_ur_x, coordinate_ur_y)\n",
        "\n",
        "    rel_coord_x = (coordinates[0] - coordinate_ll_x) / dx\n",
        "    rel_coord_y = (coordinates[1] - coordinate_ll_y) / dy\n",
        "    value = (rtype, rel_coord_x, rel_coord_y)\n",
        "    d1[tile].append(value)\n",
        "    d2[rtype].add(tile)\n",
        "\n",
        "def coord_is_in_bb(coord, bb):\n",
        "    x_min = bb[0]\n",
        "    y_min = bb[1]\n",
        "    x_max = bb[2]\n",
        "    y_max = bb[3]\n",
        "    return coord[0] > x_min and coord[0] < x_max and coord[1] > y_min and coord[1] < y_max\n",
        "\n",
        "def retrieve_roadtype(elem):\n",
        "    return elem[PROPERTIES_KEY][WEGSOORT_KEY]\n",
        "\n",
        "def retrieve_coordinates(elem):\n",
        "    return elem[GEOMETRY_KEY][COORDINATES_KEY]\n",
        "\n",
        "def eucledian_distance(p1, p2):\n",
        "    diff = np.array(p2)-np.array(p1)\n",
        "    return np.linalg.norm(diff)\n",
        "\n",
        "def calculate_intermediate_points(p1, p2, no_points):\n",
        "    dx = (p2[0] - p1[0]) / (no_points + 1)\n",
        "    dy = (p2[1] - p1[1]) / (no_points + 1)\n",
        "    return [[p1[0] + i * dx, p1[1] +  i * dy] for i in range(1, no_points+1)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrkZYwBzCLHu"
      },
      "source": [
        "# 4. Map contents of shapefile to the tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qj-wiRhqCLHv"
      },
      "outputs": [],
      "source": [
        "filename_wegvakken = json_filename\n",
        "dict_wegvakken = json.load(open(filename_wegvakken))[FEATURES_KEY]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "heWDhN5yCLH1"
      },
      "outputs": [],
      "source": [
        "d_tile_contents = defaultdict(list)\n",
        "d_roadtype_tiles = defaultdict(set)\n",
        "\n",
        "for elem in dict_wegvakken:\n",
        "    coordinates = retrieve_coordinates(elem)\n",
        "    rtype = retrieve_roadtype(elem)\n",
        "    coordinates_in_bb = [coord for coord in coordinates if coord_is_in_bb(coord, bounding_box)]\n",
        "    if len(coordinates_in_bb)==1:\n",
        "        coord = coordinates_in_bb[0]\n",
        "        add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
        "    if len(coordinates_in_bb)>1:\n",
        "        add_to_dict(d_tile_contents, d_roadtype_tiles, coordinates_in_bb[0], rtype)\n",
        "        for ii in range(1,len(coordinates_in_bb)):\n",
        "            previous_coord = coordinates_in_bb[ii-1]\n",
        "            coord = coordinates_in_bb[ii]\n",
        "            add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
        "\n",
        "            dist = eucledian_distance(previous_coord, coord)\n",
        "            no_intermediate_points = int(dist*POINTS_PER_METER)\n",
        "            intermediate_coordinates = calculate_intermediate_points(previous_coord, coord, no_intermediate_points)\n",
        "            for intermediate_coord in intermediate_coordinates:\n",
        "                add_to_dict(d_tile_contents, d_roadtype_tiles, intermediate_coord, rtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BZQd75yCLH3"
      },
      "source": [
        "# 4b. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bGU5QbFCLH3"
      },
      "outputs": [],
      "source": [
        "x0 = 93000\n",
        "y0 = 430000\n",
        "\n",
        "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
        "\n",
        "for ii in range(0,11):\n",
        "    for jj in range(0,11):\n",
        "        ll_x = x0 + ii*dx\n",
        "        ll_y = y0 + jj*dy\n",
        "        ur_x = ll_x + dx\n",
        "        ur_y = ll_y + dy\n",
        "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
        "        filename = INPUT_FOLDER_TILES + tile\n",
        "        tile_contents = d_tile_contents[tile]\n",
        "\n",
        "        ax = axarr[10-jj, ii]\n",
        "        image = ndimage.imread(filename)\n",
        "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(rgb_image)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        for elem in tile_contents:\n",
        "            color = dict_roadtype_to_color[elem[0]]\n",
        "            x = elem[1]*256\n",
        "            y = (1-elem[2])*256\n",
        "            ax.scatter(x,y,c=color,s=10)\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8jq8YEqCLH7"
      },
      "outputs": [],
      "source": [
        "x0 = 94400\n",
        "y0 = 432000\n",
        "POINTS_PER_METER = 0\n",
        "\n",
        "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
        "\n",
        "for ii in range(0,11):\n",
        "    for jj in range(0,11):\n",
        "        ll_x = x0 + ii*dx\n",
        "        ll_y = y0 + jj*dy\n",
        "        ur_x = ll_x + dx\n",
        "        ur_y = ll_y + dy\n",
        "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
        "        filename = INPUT_FOLDER_TILES + tile\n",
        "        tile_contents = d_tile_contents[tile]\n",
        "\n",
        "        ax = axarr[10-jj, ii]\n",
        "        image = ndimage.imread(filename)\n",
        "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(rgb_image)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        for elem in tile_contents:\n",
        "            color = dict_roadtype_to_color[elem[0]]\n",
        "            x = elem[1]*256\n",
        "            y = (1-elem[2])*256\n",
        "            ax.scatter(x,y,c=color,s=10)\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTMzl5nTCLH-"
      },
      "source": [
        "# 4c. Some statistics about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn3YyO1MCLID"
      },
      "outputs": [],
      "source": [
        "print(\"There are {} tiles containing roads.\".format(len(d_tile_contents.keys())))\n",
        "\n",
        "for rtype in d_roadtype_tiles.keys():\n",
        "    roadtype = dict_roadtype[rtype]\n",
        "    no_tiles = len(d_roadtype_tiles[rtype])\n",
        "    print(\"Of roadtype {} ({}) there are {} tiles.\".format(rtype, roadtype, no_tiles))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xgYxRIACLIG"
      },
      "source": [
        "# 5. Prepare dataset for CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eM3pf6gSCLIJ"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions, labels):\n",
        "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
        "\n",
        "def onehot_encode_labels(labels):\n",
        "    list_possible_labels = list(np.unique(labels))\n",
        "    encoded_labels = map(lambda x: list_possible_labels.index(x), labels)\n",
        "    return encoded_labels\n",
        "\n",
        "def randomize(dataset, labels1, labels2, labels3):\n",
        "    permutation = np.random.permutation(dataset.shape[0])\n",
        "    randomized_dataset = dataset[permutation, :, :, :]\n",
        "    randomized_labels1 = labels1[permutation]\n",
        "    randomized_labels2 = labels2[permutation]\n",
        "    randomized_labels3 = labels3[permutation]\n",
        "    return randomized_dataset, randomized_labels1, randomized_labels2, randomized_labels3\n",
        "\n",
        "def one_hot_encode(np_array, num_unique_labels):\n",
        "    return (np.arange(num_unique_labels) == np_array[:,None]).astype(np.float32)\n",
        "\n",
        "def reformat_data(dataset, labels1, labels2, labels3):\n",
        "    dataset, labels1, labels2, labels3 = randomize(dataset, labels1, labels2, labels3)\n",
        "    num_unique_labels1 = len(np.unique(labels1))\n",
        "    num_unique_labels2 = len(np.unique(labels2))\n",
        "    labels1 = one_hot_encode(labels1, num_unique_labels1)\n",
        "    labels2 = one_hot_encode(labels2, num_unique_labels2)\n",
        "    return dataset, labels1, labels2, labels3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLgA7_eVCLIK"
      },
      "outputs": [],
      "source": [
        "image_width = 256\n",
        "image_height = 256\n",
        "image_depth = 3\n",
        "total_no_images = 10000\n",
        "\n",
        "image_files = os.listdir(INPUT_FOLDER_TILES)\n",
        "\n",
        "dataset = np.ndarray(shape=(total_no_images, image_width, image_height, image_depth), dtype=np.float32)\n",
        "labels_roadtype = []\n",
        "labels_roadpresence = np.ndarray(total_no_images, dtype=np.float32)\n",
        "labels_filename = []\n",
        "\n",
        "for counter, image in enumerate(image_files):\n",
        "    filename = INPUT_FOLDER_TILES + image\n",
        "    labels_filename.append(image)\n",
        "    if image in list(d_tile_contents.keys()):\n",
        "        tile_contents = d_tile_contents[image]\n",
        "        roadtypes = sorted(list(set([elem[0] for elem in tile_contents])))\n",
        "        roadtype = \"_\".join(roadtypes)\n",
        "        labels_roadpresence[counter] = 1\n",
        "    else:\n",
        "        roadtype = ''\n",
        "        labels_roadpresence[counter] = 0\n",
        "    labels_roadtype.append(roadtype)\n",
        "\n",
        "    image_data = ndimage.imread(filename).astype(np.float32)\n",
        "    dataset[counter, :, :] = image_data\n",
        "    if counter % 1000 == 0:\n",
        "        print(\"{} images have been loaded.\".format(counter))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS4WUzioCLIM"
      },
      "outputs": [],
      "source": [
        "labels_filename = np.array(labels_filename)\n",
        "labels_roadtype_ohe = np.array(list(onehot_encode_labels(labels_roadtype)))\n",
        "print(\"Randomizing dataset...\")\n",
        "dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename = reformat_data(dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o7nJ7Z_CLIM"
      },
      "outputs": [],
      "source": [
        "start_train_dataset = 0\n",
        "start_valid_dataset = 1200\n",
        "start_test_dataset = 1600\n",
        "total_no_images = 2000\n",
        "\n",
        "output_pickle_file = './data/sattelite_dataset.pickle'\n",
        "\n",
        "f = open(output_pickle_file, 'wb')\n",
        "save = {\n",
        "'train_dataset': dataset[start_train_dataset:start_valid_dataset,:,:,:],\n",
        "'train_labels_roadtype': labels_roadtype[start_train_dataset:start_valid_dataset],\n",
        "'train_labels_roadpresence': labels_roadpresence[start_train_dataset:start_valid_dataset],\n",
        "'train_labels_filename': labels_filename[start_train_dataset:start_valid_dataset],\n",
        "'valid_dataset': dataset[start_valid_dataset:start_test_dataset,:,:,:],\n",
        "'valid_labels_roadtype': labels_roadtype[start_valid_dataset:start_test_dataset],\n",
        "'valid_labels_roadpresence': labels_roadpresence[start_valid_dataset:start_test_dataset],\n",
        "'valid_labels_filename': labels_filename[start_valid_dataset:start_test_dataset],\n",
        "'test_dataset': dataset[start_test_dataset:total_no_images,:,:,:],\n",
        "'test_labels_roadtype': labels_roadtype[start_test_dataset:total_no_images],\n",
        "'test_labels_roadpresence': labels_roadpresence[start_test_dataset:total_no_images],\n",
        "'test_labels_filename': labels_filename[start_test_dataset:total_no_images]\n",
        "}\n",
        "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
        "f.close()\n",
        "\n",
        "print(\"\\nsaved dataset to {}\".format(output_pickle_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4YtxJq8CLIO"
      },
      "source": [
        "# 6. The Convolutional neural network part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iHpezYZYCLIO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from cnn_models.vggnet16 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P185ByI8CLIP"
      },
      "outputs": [],
      "source": [
        "pickle_file = './data/sattelite_dataset.pickle'\n",
        "f = open(pickle_file, 'rb')\n",
        "save = pickle.load(f)\n",
        "\n",
        "train_dataset = save['train_dataset'].astype(dtype = np.float32)\n",
        "train_labels = save['train_labels_roadpresence'].astype(dtype = np.float32)\n",
        "valid_dataset = save['valid_dataset'].astype(dtype = np.float32)\n",
        "valid_labels = save['valid_labels_roadpresence'].astype(dtype = np.float32)\n",
        "test_dataset = save['test_dataset'].astype(dtype = np.float32)\n",
        "test_labels = save['test_labels_roadpresence'].astype(dtype = np.float32)\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b2JT_EjkCLIQ"
      },
      "outputs": [],
      "source": [
        "num_labels = len(np.unique(train_labels))\n",
        "image_width = 256\n",
        "image_height = 256\n",
        "image_depth = 3\n",
        "num_steps = 501\n",
        "display_step = 10\n",
        "learning_rate = 0.0001\n",
        "batch_size = 16\n",
        "lambda_loss_amount = 0.0015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IAtk6UtCCLIQ"
      },
      "outputs": [],
      "source": [
        "train_accuracies, test_accuracies, valid_accuracies = [], [], []\n",
        "\n",
        "print(\"STARTING WITH SATTELITE\")\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    #1) First we put the input data in a tensorflow friendly form.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
        "    tf_test_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n",
        "    tf_test_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n",
        "    tf_valid_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
        "\n",
        "    #2) Then, the weight matrices and bias vectors are initialized\n",
        "    variables = variables_vggnet16()\n",
        "\n",
        "    #3. The model used to calculate the logits (predicted labels)\n",
        "    model = model_vggnet16\n",
        "\n",
        "    logits = model(tf_train_dataset, variables)\n",
        "\n",
        "    #4. then we compute the softmax cross entropy between the logits and the (actual) labels\n",
        "    l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + l2\n",
        "\n",
        "    #learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.85, staircase=True)\n",
        "    #5. The optimizer is used to calculate the gradients of the loss function\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "    # Predictions for the training, validation, and test data.\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    test_prediction = tf.nn.softmax(model(tf_test_dataset, variables))\n",
        "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, variables))\n",
        "\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    test_counter = 0\n",
        "    tf.global_variables_initializer().run()\n",
        "    print('Initialized with learning_rate', learning_rate, \" model \", ii)\n",
        "    for step in range(num_steps):\n",
        "        #Since we are using stochastic gradient descent, we are selecting  small batches from the training dataset,\n",
        "        #and training the convolutional neural network each time with a batch.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :,  :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        train_accuracy = accuracy(predictions, batch_labels)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        if step % display_step == 0:\n",
        "            offset2 = (test_counter * batch_size) % (test_labels.shape[0] - batch_size)\n",
        "            test_dataset_batch = test_dataset[offset2:(offset2 + batch_size), :, :]\n",
        "            test_labels_batch = test_labels[offset2:(offset2 + batch_size), :]\n",
        "            feed_dict2 = {tf_test_dataset : test_dataset_batch, tf_test_labels : test_labels_batch}\n",
        "\n",
        "            test_prediction_ = session.run(test_prediction, feed_dict=feed_dict2)\n",
        "            test_accuracy = accuracy(test_prediction_, test_labels_batch)\n",
        "            test_accuracies.append(test_accuracy)\n",
        "\n",
        "            valid_dataset_batch = valid_dataset[offset2:(offset2 + batch_size), :, :]\n",
        "            valid_labels_batch = valid_labels[offset2:(offset2 + batch_size), :]\n",
        "            feed_dict3 = {tf_valid_dataset : valid_dataset_batch, tf_valid_labels : valid_labels_batch}\n",
        "\n",
        "            valid_prediction_ = session.run(valid_prediction, feed_dict=feed_dict3)\n",
        "            valid_accuracy = accuracy(valid_prediction_, valid_labels_batch)\n",
        "            valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "            message = \"step {:04d} : loss is {:06.2f}, accuracy on training set {:02.2f} %, accuracy on test set {:02.2f} accuracy on valid set {:02.2f} %\".format(step, l, train_accuracy, test_accuracy, valid_accuracy)\n",
        "            print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOHcfUkNCLIa"
      },
      "source": [
        "# Visualizing Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wZPTZl9eCLIa"
      },
      "outputs": [],
      "source": [
        "def average_points(points, stepsize = 10):\n",
        "    averaged_points = []\n",
        "    for ii in range(stepsize,len(points),stepsize):\n",
        "        subsection  = points[ii-stepsize:ii]\n",
        "        average = np.nanmean(subsection)\n",
        "        averaged_points.append(average)\n",
        "    return averaged_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTXAS95sCLIb"
      },
      "outputs": [],
      "source": [
        "num_steps = 501\n",
        "ylimit = [0,100]\n",
        "labels = ['Train accuracy', 'Test accuracy', 'Validation accuracy']\n",
        "ylabel = \"Accuracy [%]\"\n",
        "xlabel = \"Number of Iterations\"\n",
        "title = \"Accuracy of road detection in Aerial Images\"\n",
        "colors = ['r', 'g', 'b']\n",
        "\n",
        "list_accuracies = [train_accuracies, test_accuracies, valid_accuracies]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.set_ylim(ylimit)\n",
        "ax.set_ylabel(ylabel, fontsize=16)\n",
        "ax.set_xlabel(xlabel, fontsize=16)\n",
        "ax.set_title(title, fontsize=20)\n",
        "\n",
        "\n",
        "for ii, accuracies in enumerate(list_accuracies):\n",
        "    color = colors[ii]\n",
        "    label = labels[ii]\n",
        "    if ii > 0:\n",
        "        y_values = accuracies\n",
        "        x_values = range(0,num_steps, 10)\n",
        "        ax.plot(x_values, y_values, '.-{}'.format(color), label = label)\n",
        "    else:\n",
        "        y_values_ = accuracies\n",
        "        y_values = average_points(y_values_, 5)\n",
        "        x_values = range(1,len(y_values_),5)\n",
        "        ax.plot(x_values, y_values, '.{}'.format(color), label = label)\n",
        "ax.legend(loc='lower right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}